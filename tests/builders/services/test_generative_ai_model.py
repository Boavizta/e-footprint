import unittest
from unittest.mock import Mock, patch

from efootprint.abstract_modeling_classes.empty_explainable_object import EmptyExplainableObject
from efootprint.abstract_modeling_classes.modeling_update import ModelingUpdate
from efootprint.constants.units import u
from efootprint.abstract_modeling_classes.source_objects import SourceValue, SourceObject
from efootprint.core.hardware.hardware_base import InsufficientCapacityError
from efootprint.core.hardware.server import Server
from efootprint.core.hardware.gpu_server import GPUServer
from efootprint.builders.services.generative_ai_ecologits import GenAIModel, GenAIJob
from efootprint.core.hardware.storage import Storage


class TestGenAIModel(unittest.TestCase):
    def setUp(self):
        self.mock_server = Mock(spec=GPUServer)
        self.mock_server.name = "Test Server"
        self.mock_server.ram_per_gpu = SourceValue(80 * u.GB_ram)
        self.mock_server.systems = []
        self.mock_server.modeling_objects_whose_attributes_depend_directly_on_me = []
        self.model_name = SourceObject("open-mistral-7b")
        self.provider = SourceObject("mistralai")
        self.genai_model = GenAIModel.from_defaults(
            name="Test GenAI", provider=self.provider, model_name=self.model_name, server=self.mock_server)
        self.genai_model.trigger_modeling_updates = False

    def test_raises_error_if_wrong_model_name(self):
        with self.assertRaises(ValueError):
            GenAIModel.from_defaults(
                name="Test GenAI", provider=SourceObject("mistralai"), model_name=SourceObject("wrong-model"),
                server=self.mock_server)

    def test_installable_on(self):
        """Test that the installable_on property returns the server type."""
        self.assertEqual(GenAIModel.installable_on(), [GPUServer])

    def test_compatible_jobs(self):
        """Test that the compatible_jobs property returns the GenAIJob class."""
        self.assertEqual(GenAIModel.compatible_jobs(), [GenAIJob])

    def test_initialization_fails_if_server_not_generated_by_gpu_server(self):
        """Test that the model initialization fails if the server is not generated by a GPU server builder."""
        mock_server = Mock(spec=Server)
        mock_server.name = "Test Server"
        with self.assertRaises(TypeError):
            GenAIModel.from_defaults(
                name="Test GenAI", provider=SourceObject(self.provider.value),
                model_name=SourceObject(self.model_name.value), server=mock_server)

    def test_update_active_params(self):
        """Test updating active parameters."""
        mock_model = Mock()
        mock_model.architecture.parameters = 2
        with patch("efootprint.builders.services.generative_ai_ecologits.models.find_model",
                   return_value=mock_model):
            self.genai_model.update_active_params()
            self.assertEqual(self.genai_model.active_params.value, 2e9 * u.dimensionless)

    def test_update_total_params(self):
        """Test updating total parameters."""
        mock_model = Mock()
        mock_model.architecture.parameters = 5
        with patch("efootprint.builders.services.generative_ai_ecologits.models.find_model",
                   return_value=mock_model):
            self.genai_model.update_total_params()
            self.assertEqual(self.genai_model.total_params.value, 5e9 * u.dimensionless)

    def test_update_base_ram_consumption(self):
        """Test updating base RAM consumption."""
        self.genai_model.total_params = SourceValue(5e9 * u.dimensionless)
        self.genai_model.update_base_ram_consumption()
        expected_ram = 1.2 * 5e9 * 16 * u.dimensionless
        self.assertEqual(expected_ram.to(u.GB), self.genai_model.base_ram_consumption.value)

    def test_active_and_total_params_for_all_possible_list_input_values(self):
        for provider in GenAIModel.list_values["provider"]:
            for model_name in GenAIModel.conditional_list_values["model_name"]["conditional_list_values"][provider]:
                with patch.object(self.genai_model.provider, "value", provider.value), \
                    patch.object(self.genai_model.model_name, "value", model_name.value):
                        self.genai_model.update_total_params()
                        self.genai_model.update_active_params()

    def test_setting_provider_after_init_raises_value_error(self):
        self.genai_model.trigger_modeling_updates = True
        with self.assertRaises(ValueError):
            self.genai_model.provider = SourceObject("openai")

    def test_changing_provider_and_model_name_triggers_modeling_updates(self):
        self.genai_model.trigger_modeling_updates = True
        initial_total_params = self.genai_model.total_params
        initial_active_params = self.genai_model.active_params
        ModelingUpdate([[self.genai_model.provider, SourceObject("openai")],
                        [self.genai_model.model_name, SourceObject("gpt-4o")]])
        self.assertNotEqual(initial_total_params, self.genai_model.total_params)
        self.assertNotEqual(initial_active_params, self.genai_model.active_params)
        ModelingUpdate([[self.genai_model.provider, SourceObject("mistralai")],
                        [self.genai_model.model_name, SourceObject("open-mistral-7b")]])
        self.assertEqual(initial_total_params, self.genai_model.total_params)
        self.assertEqual(initial_active_params, self.genai_model.active_params)

    def test_setattr_passes_check_input_validity_parameter_to_super_method(self):
        with patch("efootprint.builders.services.generative_ai_ecologits.Service.__setattr__") as mock_setattr:
            self.genai_model.__setattr__("provider", SourceObject("openai"), check_input_validity=False)
            mock_setattr.assert_called_once_with("provider", SourceObject("openai"), check_input_validity=False)

    def test_installing_too_big_model_raises_error(self):
        server = GPUServer.from_defaults("Test Server", storage=Storage.ssd())
        with self.assertRaises(InsufficientCapacityError) as context:
            GenAIModel.from_defaults(
                name="Test GenAI", provider=SourceObject("openai"), model_name=SourceObject("gpt-4"), server=server)
        self.assertIn(
            "Test Server has available RAM capacity of 320.0 gigabyte_ram but is asked for 4224.0 gigabyte_ram", str(context.exception))


class TestGenAIJob(unittest.TestCase):
    def setUp(self):
        self.service = Mock(spec=GenAIModel)
        self.server = Mock(spec=GPUServer)
        self.service.bits_per_token = EmptyExplainableObject()
        self.service.gpu_latency_alpha = EmptyExplainableObject()
        self.service.gpu_latency_beta = EmptyExplainableObject()
        self.service.active_params = EmptyExplainableObject()
        self.service.llm_memory_factor = EmptyExplainableObject()
        self.service.nb_of_bits_per_parameter = EmptyExplainableObject()
        self.server.ram_per_gpu = EmptyExplainableObject()
        self.server.name = "Test Server"
        self.service.server = self.server
        self.job = GenAIJob.from_defaults("GenAI job", service=self.service)
        self.job.trigger_modeling_updates = False

    def test_compatible_services(self):
        self.assertEqual(GenAIJob.compatible_services(), [GenAIModel])

    def test_update_output_token_weights(self):
        with patch.object(self.job, "output_token_count", SourceValue(1000 * u.dimensionless)), \
                patch.object(self.service, "bits_per_token", SourceValue(16 * u.dimensionless)):
            self.job.update_output_token_weights()
            self.assertEqual(self.job.output_token_weights.value, 16000 * u.dimensionless)

    def test_update_data_stored(self):
        with patch.object(self.job, "output_token_weights", SourceValue(16000 * u.dimensionless)):
            self.job.update_data_stored()
            self.assertEqual(self.job.data_stored.value, 16000 * u.dimensionless + 100 * u.kB)

    def test_update_data_transferred(self):
        with patch.object(self.job, "output_token_weights", SourceValue(16000 * u.dimensionless)):
            self.job.update_data_transferred()
            self.assertEqual(self.job.data_transferred.value, 16000 * u.dimensionless + 100 * u.kB)

    def test_update_request_duration(self):
        with patch.object(self.service, "gpu_latency_alpha", SourceValue(1e-2 * u.ns)), \
                patch.object(self.service, "gpu_latency_beta", SourceValue(0.1 * u.s)), \
                patch.object(self.service, "active_params", SourceValue(100e9 * u.dimensionless)), \
                patch.object(self.job, "output_token_count", SourceValue(1 * u.dimensionless)):
            self.job.update_request_duration()
            self.assertTrue(abs(self.job.request_duration.value - 1.1 * u.s) < 1e-10 * u.s)

    def test_update_compute_needed(self):
        with patch.object(self.service, "llm_memory_factor", SourceValue(2 * u.dimensionless)), \
                patch.object(self.service, "nb_of_bits_per_parameter", SourceValue(10 * u.dimensionless)), \
                patch.object(self.service, "active_params", SourceValue(100e9 * u.dimensionless)), \
                patch.object(self.server, "ram_per_gpu", SourceValue(100e9 * u.bit_ram / u.gpu)):
            self.job.update_compute_needed()
            self.assertEqual(self.job.compute_needed.value, 20 * u.gpu)


if __name__ == "__main__":
    unittest.main()
