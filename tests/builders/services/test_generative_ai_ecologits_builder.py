import unittest
from unittest.mock import Mock, patch

from efootprint.abstract_modeling_classes.explainable_objects import EmptyExplainableObject
from efootprint.constants.units import u
from efootprint.abstract_modeling_classes.source_objects import SourceValue, SourceObject
from efootprint.core.hardware.server import Server
from efootprint.builders.hardware.gpu_server_builder import GPUServer
from efootprint.builders.services.generative_ai_ecologits import GenAIModel, GenAIJob


class TestGenAIModel(unittest.TestCase):
    def setUp(self):
        self.mock_server = Mock(spec=GPUServer)
        self.mock_server.contextual_modeling_obj_containers = []
        self.mock_server.name = "Test Server"
        self.mock_server.ram_per_gpu = SourceValue(80 * u.GB)
        self.model_name = SourceObject("open-mistral-7b")
        self.provider = SourceObject("mistralai")
        self.genai_model = GenAIModel.from_defaults(
            name="Test GenAI", provider=self.provider, model_name=self.model_name, server=self.mock_server)

    def test_initialization_fails_if_server_not_generated_by_gpu_server_builder(self):
        """Test that the model initialization fails if the server is not generated by a GPU server builder."""
        mock_server = Mock(spec=Server)
        mock_server.contextual_modeling_obj_containers = []
        mock_server.name = "Test Server"
        with self.assertRaises(PermissionError):
            GenAIModel.from_defaults(
                name="Test GenAI", provider=SourceObject(self.provider.value),
                model_name=SourceObject(self.model_name.value), server=mock_server)

    def test_update_active_params(self):
        """Test updating active parameters."""
        mock_model = Mock()
        mock_model.architecture.parameters = 2
        with patch("efootprint.builders.services.generative_ai_ecologits.models.find_model",
                   return_value=mock_model):
            self.genai_model.update_active_params()
            self.assertEqual(self.genai_model.active_params.value, 2e9 * u.dimensionless)

    def test_update_total_params(self):
        """Test updating total parameters."""
        mock_model = Mock()
        mock_model.architecture.parameters = 5
        with patch("efootprint.builders.services.generative_ai_ecologits.models.find_model",
                   return_value=mock_model):
            self.genai_model.update_total_params()
            self.assertEqual(self.genai_model.total_params.value, 5e9 * u.dimensionless)

    def test_update_base_ram_consumption(self):
        """Test updating base RAM consumption."""
        self.genai_model.total_params = SourceValue(5e9 * u.dimensionless)
        self.genai_model.update_base_ram_consumption()
        expected_ram = 1.2 * 5e9 * 16 * u.dimensionless
        self.assertEqual(expected_ram.to(u.GB), self.genai_model.base_ram_consumption.value)

    def test_update_nb_of_required_gpus_during_inference(self):
        """Test updating the number of required GPUs during inference."""
        self.genai_model.active_params = SourceValue(2e9 * u.dimensionless)
        self.genai_model.update_nb_of_required_gpus_during_inference()
        expected_gpus = (1.2 * 2e9 * 16 * u.dimensionless / (80 * u.GB)).to(u.dimensionless)
        self.assertEqual(expected_gpus, self.genai_model.nb_of_required_gpus_during_inference.value)

    def test_initialization_for_all_possible_list_input_values(self):
        for provider in GenAIModel.list_values()["provider"]:
            for model_name in GenAIModel.conditional_list_values()["model_name"]["conditional_list_values"][provider]:
                model = GenAIModel.from_defaults(
                    name="Test GenAI", provider=SourceObject(provider.value), model_name=model_name,
                    server=self.mock_server)


class TestGenAIJob(unittest.TestCase):
    def setUp(self):
        self.service = Mock(spec=GenAIModel)
        self.server = Mock(spec=GPUServer)
        self.service.contextual_modeling_obj_containers = []
        self.service.bits_per_token = EmptyExplainableObject()
        self.service.gpu_latency_alpha = EmptyExplainableObject()
        self.service.gpu_latency_beta = EmptyExplainableObject()
        self.service.active_params = EmptyExplainableObject()
        self.server.contextual_modeling_obj_containers = []
        self.server.name = "Test Server"
        self.service.server = self.server
        self.job = GenAIJob.from_defaults("GenAI job", service=self.service)
        self.job.trigger_modeling_updates = False

    def test_update_output_token_weights(self):
        with patch.object(self.job, "output_token_count", SourceValue(1000 * u.dimensionless)), \
                patch.object(self.service, "bits_per_token", SourceValue(16 * u.dimensionless)):
            self.job.update_output_token_weights()
            self.assertEqual(self.job.output_token_weights.value, 16000 * u.dimensionless)

    def test_update_data_stored(self):
        with patch.object(self.job, "output_token_weights", SourceValue(16000 * u.dimensionless)):
            self.job.update_data_stored()
            self.assertEqual(self.job.data_stored.value, 16000 * u.dimensionless + 100 * u.kB)

    def test_update_data_download(self):
        with patch.object(self.job, "output_token_weights", SourceValue(16000 * u.dimensionless)):
            self.job.update_data_download()
            self.assertEqual(self.job.data_download.value, 16000 * u.dimensionless + 100 * u.kB)

    def test_update_request_duration(self):
        with patch.object(self.service, "gpu_latency_alpha", SourceValue(1e-11 * u.s)), \
                patch.object(self.service, "gpu_latency_beta", SourceValue(0.1 * u.s)), \
                patch.object(self.service, "active_params", SourceValue(100e9 * u.dimensionless)), \
                patch.object(self.job, "output_token_count", SourceValue(1 * u.dimensionless)):
            self.job.update_request_duration()
            self.assertTrue(abs(self.job.request_duration.value - 1.1 * u.s) < 1e-10 * u.s)

if __name__ == "__main__":
    unittest.main()
