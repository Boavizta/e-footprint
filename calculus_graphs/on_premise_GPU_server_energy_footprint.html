<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork_4f448e {
                 width: 1800px;
                 height: 900px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork_4f448e" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork_4f448e');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": null, "id": "Hourly on premise GPU server energy footprint", "label": "Hourly on premise\nGPU server energy\nfootprint", "shape": "dot", "size": 15, "title": "Hourly on premise GPU server energy footprint\n=\nHourly energy consumed by on premise GPU server instances * Average carbon\nintensity of on premise GPU server electricity\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in kWh:\nfirst 10 vals [0.25, 0.24, 0.24, 0.25, 0.25, 0.24, 0.24, 0.25, 0.24, 0.25],\nlast 10 vals [0.24, 0.24, 0.24, 0.24, 0.24, 0.25, 0.24, 0.24, 0.25, 0.25] *\n100.0 gram / kilowatt_hour\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in kg:\nfirst 10 vals [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02],\nlast 10 vals [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]", "x": 0.0, "y": 1500}, {"color": null, "id": "Hourly energy consumed by on premise GPU server instances", "label": "Hourly energy\nconsumed by on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly energy consumed by on premise GPU server instances\n=\nHourly number of on premise GPU server instances * on premise GPU server idle\npower * PUE of on premise GPU server * one hour + Hourly raw number of on\npremise GPU server instances * (on premise GPU server power - on premise GPU\nserver idle power) * PUE of on premise GPU server * one hour\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] * 200.0 watt *\n1.2 dimensionless * 1.0 hour + 26298 values from 2024-12-31 23:00:00+00:00 to\n2028-01-01 17:00:00+00:00 in concurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * (1600.0 watt -\n200.0 watt) * 1.2 dimensionless * 1.0 hour\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in kWh:\nfirst 10 vals [0.25, 0.24, 0.24, 0.25, 0.25, 0.24, 0.24, 0.25, 0.24, 0.25],\nlast 10 vals [0.24, 0.24, 0.24, 0.24, 0.24, 0.25, 0.24, 0.24, 0.25, 0.25]", "x": -787.5, "y": 1350}, {"color": null, "id": "Hourly number of on premise GPU server instances", "label": "Hourly number of on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly number of on premise GPU server instances\n=\nHourly raw number of on premise GPU server instances depending on not being\nempty User defined number of on premise GPU server instances logically dependent\non Server type of on premise GPU server from e-footprint hypothesis\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] depending on not\nbeing empty no value logically dependent on on-premise\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]", "x": -1260.0, "y": 1200}, {"color": null, "id": "Hourly raw number of on premise GPU server instances", "label": "Hourly raw number of\non premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "Hourly raw number of on premise GPU server instances\n=\non premise GPU server hour by hour ram need / Available RAM per on premise GPU\nserver instance max compared with on premise GPU server hour by hour compute\nneed / Available CPU per on premise GPU server instance\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nGB_ram:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] / 270.48\ngigabyte_ram max compared with 26298 values from 2024-12-31 23:00:00+00:00 to\n2028-01-01 17:00:00+00:00 in gpu:\nfirst 10 vals [0.01, 0.01, 0.01, 0.01, 0.02, 0.0, 0.01, 0.02, 0.0, 0.01],\nlast 10 vals [0.01, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01] / 3.6 gpu\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1417.5000000000002, "y": 1050}, {"color": null, "id": "on premise GPU server hour by hour ram need", "label": "on premise GPU\nserver hour by hour\nram need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour ram need\n=\nHourly Manually defined GPU job average occurrences across usage patterns * RAM\nneeded on server on premise GPU server to process Manually defined GPU job from\ne-footprint hypothesis + no value + Hourly Generative AI model job average\noccurrences across usage patterns * No additional GPU RAM needed because model\nis already loaded in memory from e-footprint hypothesis\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * 50.0\nmegabyte_ram + no value + 26298 values from 2024-12-31 23:00:00+00:00 to\n2028-01-01 17:00:00+00:00 in concurrent:\nfirst 10 vals [0.05, 0.02, 0.02, 0.05, 0.07, 0.01, 0.02, 0.06, 0.01, 0.05],\nlast 10 vals [0.04, 0.01, 0.02, 0.04, 0.04, 0.05, 0.01, 0.04, 0.05, 0.05] * 0.0\ngigabyte\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nGB_ram:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1469.9999999999998, "y": 900}, {"color": null, "id": "Hourly Manually defined GPU job average occurrences across usage patterns", "label": "Hourly Manually\ndefined GPU job\naverage occurrences\nacross usage\npatterns", "shape": "dot", "size": 15, "title": "Hourly Manually defined GPU job average occurrences across usage patterns\n=\nAverage hourly Manually defined GPU job occurrences in usage pattern + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1500.0, "y": 750}, {"color": null, "id": "Average hourly Manually defined GPU job occurrences in usage pattern", "label": "Average hourly\nManually defined GPU\njob occurrences in\nusage pattern", "shape": "dot", "size": 15, "title": "Average hourly Manually defined GPU job occurrences in usage pattern\n=\nHourly Manually defined GPU job occurrences in usage pattern hourly occurrences\naverage Request duration of Manually defined GPU job from e-footprint hypothesis\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0] hourly\noccurrences average 1.0 second\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1482.3529411764707, "y": 600}, {"color": null, "id": "Hourly Manually defined GPU job occurrences in usage pattern", "label": "Hourly Manually\ndefined GPU job\noccurrences in usage\npattern", "shape": "dot", "size": 15, "title": "Hourly Manually defined GPU job occurrences in usage pattern\n=\nusage pattern UTC shifted by no value + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0] shifted by no\nvalue + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0]", "x": -1443.75, "y": 450}, {"color": null, "id": "usage pattern UTC", "label": "usage pattern UTC", "shape": "dot", "size": 15, "title": "usage pattern UTC\n=\nusage pattern hourly nb of visits converted to UTC from devices country timezone\nfrom user data\n=\n26298 values from 2025-01-01 00:00:00 to 2028-01-01 18:00:00 in occurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0] converted to UTC\nfrom Europe/Paris\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0]", "x": -1400.0, "y": 300}, {"color": null, "id": "usage pattern hourly nb of visits", "label": "usage pattern hourly\nnb of visits", "shape": "dot", "size": 15, "title": "usage pattern hourly nb of visits = 26298 values from 2025-01-01 00:00:00 to\n2028-01-01 18:00:00 in occurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0]", "x": -1181.25, "y": 150}, {"color": "gold", "id": "devices country timezone from user data", "label": "devices country\ntimezone from user\ndata", "shape": "dot", "size": 15, "title": "devices country timezone from user data = Europe/Paris", "x": -393.75, "y": 150}, {"color": "darkred", "id": "Request duration of Manually defined GPU job from e-footprint hypothesis", "label": "Request duration of\nManually defined GPU\njob from e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Request duration of Manually defined GPU job from e-footprint hypothesis = 1.0\nsecond", "x": -1181.25, "y": 450}, {"color": "darkred", "id": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "label": "RAM needed on server\non premise GPU\nserver to process\nManually defined GPU\njob from e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "RAM needed on server on premise GPU server to process Manually defined GPU job\nfrom e-footprint hypothesis = 50.0 megabyte_ram", "x": -1350.0, "y": 750}, {"color": null, "id": "Hourly Generative AI model job average occurrences across usage patterns", "label": "Hourly Generative AI\nmodel job average\noccurrences across\nusage patterns", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job average occurrences across usage patterns\n=\nAverage hourly Generative AI model job occurrences in usage pattern + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.05, 0.02, 0.02, 0.05, 0.07, 0.01, 0.02, 0.06, 0.01, 0.05],\nlast 10 vals [0.04, 0.01, 0.02, 0.04, 0.04, 0.05, 0.01, 0.04, 0.05, 0.05] + no\nvalue\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.05, 0.02, 0.02, 0.05, 0.07, 0.01, 0.02, 0.06, 0.01, 0.05],\nlast 10 vals [0.04, 0.01, 0.02, 0.04, 0.04, 0.05, 0.01, 0.04, 0.05, 0.05]", "x": -1200.0, "y": 750}, {"color": null, "id": "Average hourly Generative AI model job occurrences in usage pattern", "label": "Average hourly\nGenerative AI model\njob occurrences in\nusage pattern", "shape": "dot", "size": 15, "title": "Average hourly Generative AI model job occurrences in usage pattern\n=\nHourly Generative AI model job occurrences in usage pattern hourly occurrences\naverage Generative AI model job request duration\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0] hourly\noccurrences average 28154600000.0 nanosecond\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.05, 0.02, 0.02, 0.05, 0.07, 0.01, 0.02, 0.06, 0.01, 0.05],\nlast 10 vals [0.04, 0.01, 0.02, 0.04, 0.04, 0.05, 0.01, 0.04, 0.05, 0.05]", "x": -1297.058823529412, "y": 600}, {"color": null, "id": "Hourly Generative AI model job occurrences in usage pattern", "label": "Hourly Generative AI\nmodel job\noccurrences in usage\npattern", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job occurrences in usage pattern\n=\nusage pattern UTC shifted by no value + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0] shifted by no\nvalue + no value\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\noccurrence:\nfirst 10 vals [6.0, 3.0, 3.0, 7.0, 9.0, 1.0, 3.0, 8.0, 1.0, 7.0],\nlast 10 vals [5.0, 1.0, 2.0, 5.0, 5.0, 6.0, 1.0, 5.0, 6.0, 7.0]", "x": -918.75, "y": 450}, {"color": null, "id": "Generative AI model job request duration", "label": "Generative AI model\njob request duration", "shape": "dot", "size": 15, "title": "Generative AI model job request duration\n=\nGenerative AI model job output token count from e-footprint hypothesis * (GPU\nlatency per active parameter and output token from Ecologits * open-mistral-7b\nfrom mistralai nb of active parameters from Ecologits + Base GPU latency per\noutput_token from Ecologits)\n=\n1000.0 dimensionless * (0.0 nanosecond * 7300000000.0 dimensionless + 0.02\nsecond)\n=\n28154600000.0 nanosecond", "x": -656.25, "y": 450}, {"color": "darkred", "id": "Generative AI model job output token count from e-footprint hypothesis", "label": "Generative AI model\njob output token\ncount from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model job output token count from e-footprint hypothesis = 1000.0\ndimensionless", "x": -1050.0, "y": 300}, {"color": "darkred", "id": "GPU latency per active parameter and output token from Ecologits", "label": "GPU latency per\nactive parameter and\noutput token from\nEcologits", "shape": "dot", "size": 15, "title": "GPU latency per active parameter and output token from Ecologits = 0.0\nnanosecond", "x": -700.0, "y": 300}, {"color": null, "id": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "label": "open-mistral-7b from\nmistralai nb of\nactive parameters\nfrom Ecologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai nb of active parameters from Ecologits\n=\nmistralai from e-footprint hypothesis query EcoLogits data with mistralai model\nused from e-footprint hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": -350.0, "y": 300}, {"color": "darkred", "id": "mistralai from e-footprint hypothesis", "label": "mistralai from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "mistralai from e-footprint hypothesis = mistralai", "x": 393.75, "y": 150}, {"color": "darkred", "id": "mistralai model used from e-footprint hypothesis", "label": "mistralai model used\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "mistralai model used from e-footprint hypothesis = open-mistral-7b", "x": 1181.25, "y": 150}, {"color": "darkred", "id": "Base GPU latency per output_token from Ecologits", "label": "Base GPU latency per\noutput_token from\nEcologits", "shape": "dot", "size": 15, "title": "Base GPU latency per output_token from Ecologits = 0.02 second", "x": 0.0, "y": 300}, {"color": "darkred", "id": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "label": "No additional GPU\nRAM needed because\nmodel is already\nloaded in memory\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "No additional GPU RAM needed because model is already loaded in memory from\ne-footprint hypothesis = 0.0 gigabyte", "x": -1050.0, "y": 750}, {"color": null, "id": "Available RAM per on premise GPU server instance", "label": "Available RAM per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available RAM per on premise GPU server instance\n=\non premise GPU server RAM * on premise GPU server utilization rate - Occupied\nRAM per on premise GPU server instance including services\n=\n320.0 gigabyte_ram * 0.9 dimensionless - 17.52 gigabyte_ram\n=\n270.48 gigabyte_ram", "x": -1259.9999999999998, "y": 900}, {"color": null, "id": "on premise GPU server RAM", "label": "on premise GPU\nserver RAM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM\n=\non premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n* Nb gpus of on premise GPU server from e-footprint hypothesis\n=\n80.0 gigabyte_ram / gpu * 4.0 gpu\n=\n320.0 gigabyte_ram", "x": -900.0, "y": 750}, {"color": "darkred", "id": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver RAM per GPU\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n= 80.0 gigabyte_ram / gpu", "x": -1111.764705882353, "y": 600}, {"color": "darkred", "id": "Nb gpus of on premise GPU server from e-footprint hypothesis", "label": "Nb gpus of on\npremise GPU server\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Nb gpus of on premise GPU server from e-footprint hypothesis = 4.0 gpu", "x": -926.4705882352941, "y": 600}, {"color": null, "id": "on premise GPU server utilization rate", "label": "on premise GPU\nserver utilization\nrate", "shape": "dot", "size": 15, "title": "on premise GPU server utilization rate = 0.9 dimensionless", "x": -750.0, "y": 750}, {"color": null, "id": "Occupied RAM per on premise GPU server instance including services", "label": "Occupied RAM per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied RAM per on premise GPU server instance including services\n=\nBase RAM consumption of on premise GPU server from e-footprint hypothesis +\nGenerative AI model base RAM consumption\n=\n0.0 gigabyte_ram + 17.52 gigabyte\n=\n17.52 gigabyte_ram", "x": -600.0, "y": 750}, {"color": "darkred", "id": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "label": "Base RAM consumption\nof on premise GPU\nserver from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Base RAM consumption of on premise GPU server from e-footprint hypothesis = 0.0\ngigabyte_ram", "x": -741.1764705882354, "y": 600}, {"color": null, "id": "Generative AI model base RAM consumption", "label": "Generative AI model\nbase RAM consumption", "shape": "dot", "size": 15, "title": "Generative AI model base RAM consumption\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai total nb of parameters from Ecologits\n* Generative AI model nb of bits per parameter from e-footprint hypothesis\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16.0 dimensionless\n=\n17.52 gigabyte", "x": -555.8823529411765, "y": 600}, {"color": "darkred", "id": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "label": "Generative AI model\nratio between GPU\nmemory footprint and\nmodel size from\nEcologits", "shape": "dot", "size": 15, "title": "Generative AI model ratio between GPU memory footprint and model size from\nEcologits = 1.2 dimensionless", "x": -393.75, "y": 450}, {"color": null, "id": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "label": "open-mistral-7b from\nmistralai total nb\nof parameters from\nEcologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai total nb of parameters from Ecologits\n=\nmistralai from e-footprint hypothesis query EcoLogits data with mistralai model\nused from e-footprint hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": -131.25, "y": 450}, {"color": "darkred", "id": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "label": "Generative AI model\nnb of bits per\nparameter from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model nb of bits per parameter from e-footprint hypothesis = 16.0\ndimensionless", "x": 131.25, "y": 450}, {"color": null, "id": "on premise GPU server hour by hour compute need", "label": "on premise GPU\nserver hour by hour\ncompute need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour compute need\n=\nHourly Manually defined GPU job average occurrences across usage patterns * gpus\nneeded on server on premise GPU server to process Manually defined GPU job from\ne-footprint hypothesis + no value + Hourly Generative AI model job average\noccurrences across usage patterns * Generative AI model job nb of required GPUs\nduring inference\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\nconcurrent:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * 1.0 gpu + no\nvalue + 26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00\nin concurrent:\nfirst 10 vals [0.05, 0.02, 0.02, 0.05, 0.07, 0.01, 0.02, 0.06, 0.01, 0.05],\nlast 10 vals [0.04, 0.01, 0.02, 0.04, 0.04, 0.05, 0.01, 0.04, 0.05, 0.05] * 0.22\ngpu\n=\n26298 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in gpu:\nfirst 10 vals [0.01, 0.01, 0.01, 0.01, 0.02, 0.0, 0.01, 0.02, 0.0, 0.01],\nlast 10 vals [0.01, 0.0, 0.0, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01]", "x": -1050.0, "y": 900}, {"color": "darkred", "id": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "label": "gpus needed on\nserver on premise\nGPU server to\nprocess Manually\ndefined GPU job from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "gpus needed on server on premise GPU server to process Manually defined GPU job\nfrom e-footprint hypothesis = 1.0 gpu", "x": -450.0, "y": 750}, {"color": null, "id": "Generative AI model job nb of required GPUs during inference", "label": "Generative AI model\njob nb of required\nGPUs during\ninference", "shape": "dot", "size": 15, "title": "Generative AI model job nb of required GPUs during inference\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai nb of active parameters from\nEcologits * Generative AI model nb of bits per parameter from e-footprint\nhypothesis / on premise GPU server RAM per GPU from Estimating the Carbon\nFootprint of BLOOM\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16.0 dimensionless / 80.0\ngigabyte_ram / gpu\n=\n0.22 gpu", "x": -300.0, "y": 750}, {"color": null, "id": "Available CPU per on premise GPU server instance", "label": "Available CPU per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available CPU per on premise GPU server instance\n=\nNb gpus of on premise GPU server from e-footprint hypothesis * on premise GPU\nserver utilization rate - Occupied CPU per on premise GPU server instance\nincluding services\n=\n4.0 gpu * 0.9 dimensionless - 0.0 gpu\n=\n3.6 gpu", "x": -840.0, "y": 900}, {"color": null, "id": "Occupied CPU per on premise GPU server instance including services", "label": "Occupied CPU per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied CPU per on premise GPU server instance including services\n=\nBase gpu consumption of on premise GPU server from e-footprint hypothesis + + 0\n(no value)\n=\n0.0 gpu + + 0 (no value)\n=\n0.0 gpu", "x": 0.0, "y": 750}, {"color": "darkred", "id": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "label": "Base gpu consumption\nof on premise GPU\nserver from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Base gpu consumption of on premise GPU server from e-footprint hypothesis = 0.0\ngpu", "x": 0.0, "y": 600}, {"color": null, "id": "no value", "label": "no value", "shape": "dot", "size": 15, "title": "no value = no value", "x": 185.29411764705884, "y": 600}, {"color": null, "id": "User defined number of on premise GPU server instances", "label": "User defined number\nof on premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "User defined number of on premise GPU server instances = no value", "x": -1102.5, "y": 1050}, {"color": "darkred", "id": "Server type of on premise GPU server from e-footprint hypothesis", "label": "Server type of on\npremise GPU server\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Server type of on premise GPU server from e-footprint hypothesis = on-premise", "x": -787.5, "y": 1050}, {"color": null, "id": "on premise GPU server idle power", "label": "on premise GPU\nserver idle power", "shape": "dot", "size": 15, "title": "on premise GPU server idle power\n=\non premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM * Nb gpus of on premise GPU server from e-footprint hypothesis\n=\n50.0 watt / gpu * 4.0 gpu\n=\n200.0 watt", "x": -630.0, "y": 1200}, {"color": "darkred", "id": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU idle\npower from\nEstimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM = 50.0 watt / gpu", "x": -472.50000000000006, "y": 1050}, {"color": null, "id": "PUE of on premise GPU server", "label": "PUE of on premise\nGPU server", "shape": "dot", "size": 15, "title": "PUE of on premise GPU server = 1.2 dimensionless", "x": 0.0, "y": 1200}, {"color": null, "id": "on premise GPU server power", "label": "on premise GPU\nserver power", "shape": "dot", "size": 15, "title": "on premise GPU server power\n=\non premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM *\nNb gpus of on premise GPU server from e-footprint hypothesis\n=\n400.0 watt / gpu * 4.0 gpu\n=\n1600.0 watt", "x": 1260.0, "y": 1200}, {"color": "darkred", "id": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU power\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM =\n400.0 watt / gpu", "x": 1102.5, "y": 1050}, {"color": null, "id": "Average carbon intensity of on premise GPU server electricity", "label": "Average carbon\nintensity of on\npremise GPU server\nelectricity", "shape": "dot", "size": 15, "title": "Average carbon intensity of on premise GPU server electricity = 100.0 gram /\nkilowatt_hour", "x": 787.5, "y": 1350}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Hourly energy consumed by on premise GPU server instances", "to": "Hourly on premise GPU server energy footprint"}, {"arrows": "to", "from": "Hourly number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "User defined number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Server type of on premise GPU server from e-footprint hypothesis", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server idle power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "PUE of on premise GPU server", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "on premise GPU server power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server power"}, {"arrows": "to", "from": "Average carbon intensity of on premise GPU server electricity", "to": "Hourly on premise GPU server energy footprint"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": false,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>