<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork_cf230d {
                 width: 1800px;
                 height: 900px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork_cf230d" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork_cf230d');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": null, "id": "Hourly energy consumed by on premise GPU server instances", "label": "Hourly energy\nconsumed by on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly energy consumed by on premise GPU server instances\n=\nHourly number of on premise GPU server instances * on premise GPU server idle\npower * PUE of on premise GPU server * one hour + Hourly raw number of on\npremise GPU server instances * (on premise GPU server power - on premise GPU\nserver idle power) * PUE of on premise GPU server * one hour\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] * 200.0 watt *\n1.2 dimensionless * 1 hour + 26299 values from 2024-12-31 23:00:00 to 2028-01-01\n17:00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * (1600.0 watt -\n200.0 watt) * 1.2 dimensionless * 1 hour\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in kWh:\nfirst 10 vals [0.25, 0.25, 0.24, 0.24, 0.24, 0.25, 0.25, 0.24, 0.25, 0.24],\nlast 10 vals [0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.25, 0.24]", "x": 0.0, "y": 1650}, {"color": null, "id": "Hourly number of on premise GPU server instances", "label": "Hourly number of on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly number of on premise GPU server instances\n=\nHourly number of on premise GPU server instances logically dependent on Server\ntype of on premise GPU server from hypothesis\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] logically\ndependent on on-premise\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]", "x": -964.2857142857143, "y": 1350}, {"color": null, "id": "Hourly raw number of on premise GPU server instances", "label": "Hourly raw number of\non premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "Hourly raw number of on premise GPU server instances\n=\nRaw nb of on premise GPU server instances based on RAM alone max compared with\nRaw nb of on premise GPU server instances based on CPU alone\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] max compared\nwith 26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -937.5, "y": 1200}, {"color": null, "id": "Raw nb of on premise GPU server instances based on RAM alone", "label": "Raw nb of on premise\nGPU server instances\nbased on RAM alone", "shape": "dot", "size": 15, "title": "Raw nb of on premise GPU server instances based on RAM alone\n=\non premise GPU server hour by hour ram need / Available RAM per on premise GPU\nserver instance\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in GB:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] / 270.48\ngigabyte\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1012.5, "y": 1050}, {"color": null, "id": "on premise GPU server hour by hour ram need", "label": "on premise GPU\nserver hour by hour\nram need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour ram need\n=\nHourly Generative AI model job average occurrences across usage patterns * No\nadditional GPU RAM needed because model is already loaded in memory from\nhypothesis\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.07, 0.06, 0.05, 0.01, 0.04, 0.06, 0.05, 0.02, 0.05, 0.02],\nlast 10 vals [0.02, 0.05, 0.05, 0.02, 0.01, 0.05, 0.02, 0.02, 0.07, 0.01] * 0\ngigabyte\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in GB:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1038.4615384615383, "y": 900}, {"color": null, "id": "Hourly Generative AI model job average occurrences across usage patterns", "label": "Hourly Generative AI\nmodel job average\noccurrences across\nusage patterns", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job average occurrences across usage patterns\n=\nAverage hourly Generative AI model job occurrences in usage pattern\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.07, 0.06, 0.05, 0.01, 0.04, 0.06, 0.05, 0.02, 0.05, 0.02],\nlast 10 vals [0.02, 0.05, 0.05, 0.02, 0.01, 0.05, 0.02, 0.02, 0.07, 0.01]\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.07, 0.06, 0.05, 0.01, 0.04, 0.06, 0.05, 0.02, 0.05, 0.02],\nlast 10 vals [0.02, 0.05, 0.05, 0.02, 0.01, 0.05, 0.02, 0.02, 0.07, 0.01]", "x": -1050.0, "y": 750}, {"color": null, "id": "Average hourly Generative AI model job occurrences in usage pattern", "label": "Average hourly\nGenerative AI model\njob occurrences in\nusage pattern", "shape": "dot", "size": 15, "title": "Average hourly Generative AI model job occurrences in usage pattern\n=\nHourly Generative AI model job occurrences in usage pattern hourly occurrences\naverage Generative AI model job request duration\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1] hourly occurrences average 28.15\nsecond\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.07, 0.06, 0.05, 0.01, 0.04, 0.06, 0.05, 0.02, 0.05, 0.02],\nlast 10 vals [0.02, 0.05, 0.05, 0.02, 0.01, 0.05, 0.02, 0.02, 0.07, 0.01]", "x": -1044.642857142857, "y": 600}, {"color": null, "id": "Hourly Generative AI model job occurrences in usage pattern", "label": "Hourly Generative AI\nmodel job\noccurrences in usage\npattern", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job occurrences in usage pattern\n=\nusage pattern UTC shifted by no value\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1] shifted by no value\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1]", "x": -1000.0, "y": 450}, {"color": null, "id": "usage pattern UTC", "label": "usage pattern UTC", "shape": "dot", "size": 15, "title": "usage pattern UTC\n=\nusage pattern hourly nb of visits from hypothesis converted to UTC from devices\ncountry timezone from user data\n=\n26299 values from 2025-01-01 00:00:00 to 2028-01-01 18:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1] converted to UTC from Europe/Paris\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1]", "x": -937.5, "y": 300}, {"color": "darkred", "id": "usage pattern hourly nb of visits from hypothesis", "label": "usage pattern hourly\nnb of visits from\nhypothesis", "shape": "dot", "size": 15, "title": "usage pattern hourly nb of visits from hypothesis = 26299 values from 2025-01-01\n00:00:00 to 2028-01-01 18:00:00 in dimensionless:\nfirst 10 vals [9, 8, 6, 1, 5, 8, 7, 2, 7, 3],\nlast 10 vals [3, 6, 6, 3, 1, 6, 3, 2, 9, 1]", "x": -843.75, "y": 150}, {"color": "gold", "id": "devices country timezone from user data", "label": "devices country\ntimezone from user\ndata", "shape": "dot", "size": 15, "title": "devices country timezone from user data = Europe/Paris", "x": -281.25, "y": 150}, {"color": null, "id": "no value", "label": "no value", "shape": "dot", "size": 15, "title": "no value = no value", "x": -562.5, "y": 300}, {"color": null, "id": "Generative AI model job request duration", "label": "Generative AI model\njob request duration", "shape": "dot", "size": 15, "title": "Generative AI model job request duration\n=\nGenerative AI model job output token count from hypothesis * (GPU latency per\nactive parameter and output token from Ecologits * open-mistral-7b from\nmistralai nb of active parameters from Ecologits + Base GPU latency per\noutput_token from Ecologits)\n=\n1000 dimensionless * (0.0 second * 7300000000.0 dimensionless + 0.02 second)\n=\n28.15 second", "x": -750.0, "y": 450}, {"color": "darkred", "id": "Generative AI model job output token count from hypothesis", "label": "Generative AI model\njob output token\ncount from\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model job output token count from hypothesis = 1000 dimensionless", "x": -187.5, "y": 300}, {"color": "darkred", "id": "GPU latency per active parameter and output token from Ecologits", "label": "GPU latency per\nactive parameter and\noutput token from\nEcologits", "shape": "dot", "size": 15, "title": "GPU latency per active parameter and output token from Ecologits = 0.0 second", "x": 187.5, "y": 300}, {"color": null, "id": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "label": "open-mistral-7b from\nmistralai nb of\nactive parameters\nfrom Ecologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai nb of active parameters from Ecologits\n=\nopen-mistral-7b provider from hypothesis query EcoLogits data with mistralai\nmodel used from hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": 562.5, "y": 300}, {"color": "darkred", "id": "open-mistral-7b provider from hypothesis", "label": "open-mistral-7b\nprovider from\nhypothesis", "shape": "dot", "size": 15, "title": "open-mistral-7b provider from hypothesis = mistralai", "x": 281.25, "y": 150}, {"color": "darkred", "id": "mistralai model used from hypothesis", "label": "mistralai model used\nfrom hypothesis", "shape": "dot", "size": 15, "title": "mistralai model used from hypothesis = open-mistral-7b", "x": 843.75, "y": 150}, {"color": "darkred", "id": "Base GPU latency per output_token from Ecologits", "label": "Base GPU latency per\noutput_token from\nEcologits", "shape": "dot", "size": 15, "title": "Base GPU latency per output_token from Ecologits = 0.02 second", "x": 937.5, "y": 300}, {"color": "darkred", "id": "No additional GPU RAM needed because model is already loaded in memory from hypothesis", "label": "No additional GPU\nRAM needed because\nmodel is already\nloaded in memory\nfrom hypothesis", "shape": "dot", "size": 15, "title": "No additional GPU RAM needed because model is already loaded in memory from\nhypothesis = 0 gigabyte", "x": -900.0, "y": 750}, {"color": null, "id": "Available RAM per on premise GPU server instance", "label": "Available RAM per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available RAM per on premise GPU server instance\n=\non premise GPU server RAM * on premise GPU server utilization rate - Occupied\nRAM per on premise GPU server instance including services\n=\n320.0 gigabyte * 0.9 dimensionless - 17.52 gigabyte\n=\n270.48 gigabyte", "x": -865.3846153846152, "y": 900}, {"color": null, "id": "on premise GPU server RAM", "label": "on premise GPU\nserver RAM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM\n=\non premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n* Nb gpus of on premise GPU server from hypothesis\n=\n80.0 gigabyte / gpu * 4 gpu\n=\n320.0 gigabyte", "x": -750.0, "y": 750}, {"color": "darkred", "id": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver RAM per GPU\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n= 80.0 gigabyte / gpu", "x": -883.9285714285713, "y": 600}, {"color": "darkred", "id": "Nb gpus of on premise GPU server from hypothesis", "label": "Nb gpus of on\npremise GPU server\nfrom hypothesis", "shape": "dot", "size": 15, "title": "Nb gpus of on premise GPU server from hypothesis = 4 gpu", "x": -723.2142857142857, "y": 600}, {"color": null, "id": "on premise GPU server utilization rate", "label": "on premise GPU\nserver utilization\nrate", "shape": "dot", "size": 15, "title": "on premise GPU server utilization rate = 0.9 dimensionless", "x": -600.0, "y": 750}, {"color": null, "id": "Occupied RAM per on premise GPU server instance including services", "label": "Occupied RAM per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied RAM per on premise GPU server instance including services\n=\nBase RAM consumption of on premise GPU server from hypothesis + Generative AI\nmodel base RAM consumption\n=\n0 gigabyte + 17.52 gigabyte\n=\n17.52 gigabyte", "x": -450.0, "y": 750}, {"color": "darkred", "id": "Base RAM consumption of on premise GPU server from hypothesis", "label": "Base RAM consumption\nof on premise GPU\nserver from\nhypothesis", "shape": "dot", "size": 15, "title": "Base RAM consumption of on premise GPU server from hypothesis = 0 gigabyte", "x": -562.5, "y": 600}, {"color": null, "id": "Generative AI model base RAM consumption", "label": "Generative AI model\nbase RAM consumption", "shape": "dot", "size": 15, "title": "Generative AI model base RAM consumption\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai total nb of parameters from Ecologits\n* Generative AI model nb of bits per parameter from hypothesis\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16 dimensionless\n=\n17.52 gigabyte", "x": -401.7857142857143, "y": 600}, {"color": "darkred", "id": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "label": "Generative AI model\nratio between GPU\nmemory footprint and\nmodel size from\nEcologits", "shape": "dot", "size": 15, "title": "Generative AI model ratio between GPU memory footprint and model size from\nEcologits = 1.2 dimensionless", "x": -500.0, "y": 450}, {"color": null, "id": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "label": "open-mistral-7b from\nmistralai total nb\nof parameters from\nEcologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai total nb of parameters from Ecologits\n=\nopen-mistral-7b provider from hypothesis query EcoLogits data with mistralai\nmodel used from hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": -250.0, "y": 450}, {"color": "darkred", "id": "Generative AI model nb of bits per parameter from hypothesis", "label": "Generative AI model\nnb of bits per\nparameter from\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model nb of bits per parameter from hypothesis = 16 dimensionless", "x": 0.0, "y": 450}, {"color": null, "id": "Raw nb of on premise GPU server instances based on CPU alone", "label": "Raw nb of on premise\nGPU server instances\nbased on CPU alone", "shape": "dot", "size": 15, "title": "Raw nb of on premise GPU server instances based on CPU alone\n=\non premise GPU server hour by hour compute need / Available CPU per on premise\nGPU server instance\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in gpu:\nfirst 10 vals [0.02, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01],\nlast 10 vals [0.01, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.02, 0.0] / 3.6 gpu\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -787.5, "y": 1050}, {"color": null, "id": "on premise GPU server hour by hour compute need", "label": "on premise GPU\nserver hour by hour\ncompute need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour compute need\n=\nHourly Generative AI model job average occurrences across usage patterns *\nGenerative AI model job nb of required GPUs during inference\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in dimensionless:\nfirst 10 vals [0.07, 0.06, 0.05, 0.01, 0.04, 0.06, 0.05, 0.02, 0.05, 0.02],\nlast 10 vals [0.02, 0.05, 0.05, 0.02, 0.01, 0.05, 0.02, 0.02, 0.07, 0.01] * 0.22\ngpu\n=\n26299 values from 2024-12-31 23:00:00 to 2028-01-01 17:00:00 in gpu:\nfirst 10 vals [0.02, 0.01, 0.01, 0.0, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01],\nlast 10 vals [0.01, 0.01, 0.01, 0.01, 0.0, 0.01, 0.01, 0.0, 0.02, 0.0]", "x": -692.3076923076923, "y": 900}, {"color": null, "id": "Generative AI model job nb of required GPUs during inference", "label": "Generative AI model\njob nb of required\nGPUs during\ninference", "shape": "dot", "size": 15, "title": "Generative AI model job nb of required GPUs during inference\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai nb of active parameters from\nEcologits * Generative AI model nb of bits per parameter from hypothesis / on\npremise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16 dimensionless / 80.0\ngigabyte / gpu\n=\n0.22 gpu", "x": -300.0, "y": 750}, {"color": null, "id": "Available CPU per on premise GPU server instance", "label": "Available CPU per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available CPU per on premise GPU server instance\n=\nNb gpus of on premise GPU server from hypothesis * on premise GPU server\nutilization rate - Occupied CPU per on premise GPU server instance including\nservices\n=\n4 gpu * 0.9 dimensionless - 0 gpu\n=\n3.6 gpu", "x": -519.2307692307692, "y": 900}, {"color": null, "id": "Occupied CPU per on premise GPU server instance including services", "label": "Occupied CPU per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied CPU per on premise GPU server instance including services\n=\nBase gpu consumption of on premise GPU server from hypothesis\n=\n0 gpu\n=\n0 gpu", "x": 0.0, "y": 750}, {"color": "darkred", "id": "Base gpu consumption of on premise GPU server from hypothesis", "label": "Base gpu consumption\nof on premise GPU\nserver from\nhypothesis", "shape": "dot", "size": 15, "title": "Base gpu consumption of on premise GPU server from hypothesis = 0 gpu", "x": 241.07142857142858, "y": 600}, {"color": null, "id": "User defined number of on premise GPU server instances", "label": "User defined number\nof on premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "User defined number of on premise GPU server instances = no value", "x": -562.5, "y": 1200}, {"color": "darkred", "id": "Server type of on premise GPU server from hypothesis", "label": "Server type of on\npremise GPU server\nfrom hypothesis", "shape": "dot", "size": 15, "title": "Server type of on premise GPU server from hypothesis = on-premise", "x": -642.8571428571429, "y": 1350}, {"color": null, "id": "on premise GPU server idle power", "label": "on premise GPU\nserver idle power", "shape": "dot", "size": 15, "title": "on premise GPU server idle power\n=\non premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM * Nb gpus of on premise GPU server from hypothesis\n=\n50.0 watt / gpu * 4 gpu\n=\n200.0 watt", "x": -900.0, "y": 1500}, {"color": "darkred", "id": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU idle\npower from\nEstimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM = 50.0 watt / gpu", "x": -321.42857142857144, "y": 1350}, {"color": null, "id": "PUE of on premise GPU server", "label": "PUE of on premise\nGPU server", "shape": "dot", "size": 15, "title": "PUE of on premise GPU server = 1.2 dimensionless", "x": -450.0, "y": 1500}, {"color": null, "id": "one hour", "label": "one hour", "shape": "dot", "size": 15, "title": "one hour = 1 hour", "x": 0.0, "y": 1500}, {"color": null, "id": "on premise GPU server power", "label": "on premise GPU\nserver power", "shape": "dot", "size": 15, "title": "on premise GPU server power\n=\non premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM *\nNb gpus of on premise GPU server from hypothesis\n=\n400.0 watt / gpu * 4 gpu\n=\n1600.0 watt", "x": 900.0, "y": 1500}, {"color": "darkred", "id": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU power\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM =\n400.0 watt / gpu", "x": 642.8571428571429, "y": 1350}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Hourly number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on RAM alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on CPU alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "User defined number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Server type of on premise GPU server from hypothesis", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server idle power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "PUE of on premise GPU server", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "one hour", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on RAM alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on CPU alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b provider from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "on premise GPU server power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "on premise GPU server power"}, {"arrows": "to", "from": "on premise GPU server idle power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from hypothesis", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "PUE of on premise GPU server", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "one hour", "to": "Hourly energy consumed by on premise GPU server instances"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": false,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>