<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork_480687 {
                 width: 1800px;
                 height: 900px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork_480687" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork_480687');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": null, "id": "Hourly energy consumed by on premise GPU server instances", "label": "Hourly energy\nconsumed by on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly energy consumed by on premise GPU server instances\n=\nHourly number of on premise GPU server instances * on premise GPU server idle\npower * PUE of on premise GPU server * one hour + Hourly raw number of on\npremise GPU server instances * (on premise GPU server power - on premise GPU\nserver idle power) * PUE of on premise GPU server * one hour\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] * 200.0 watt *\n1.2 dimensionless * 1 hour + 26296 values from 2024-12-31 23:00:00+00:00 to\n2028-01-01 17:00:00+00:00 in dimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * (1600.0 watt -\n200.0 watt) * 1.2 dimensionless * 1 hour\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in kWh:\nfirst 10 vals [0.25, 0.24, 0.24, 0.24, 0.24, 0.24, 0.24, 0.25, 0.25, 0.25],\nlast 10 vals [0.25, 0.24, 0.25, 0.25, 0.25, 0.24, 0.24, 0.24, 0.25, 0.25]", "x": 0.0, "y": 1650}, {"color": null, "id": "Hourly number of on premise GPU server instances", "label": "Hourly number of on\npremise GPU server\ninstances", "shape": "dot", "size": 15, "title": "Hourly number of on premise GPU server instances\n=\nHourly number of on premise GPU server instances logically dependent on Server\ntype of on premise GPU server from e-footprint hypothesis\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] logically\ndependent on on-premise\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\nlast 10 vals [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]", "x": -1350.0, "y": 1350}, {"color": null, "id": "Hourly raw number of on premise GPU server instances", "label": "Hourly raw number of\non premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "Hourly raw number of on premise GPU server instances\n=\nRaw nb of on premise GPU server instances based on RAM alone max compared with\nRaw nb of on premise GPU server instances based on CPU alone\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] max compared\nwith 26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1312.5, "y": 1200}, {"color": null, "id": "Raw nb of on premise GPU server instances based on RAM alone", "label": "Raw nb of on premise\nGPU server instances\nbased on RAM alone", "shape": "dot", "size": 15, "title": "Raw nb of on premise GPU server instances based on RAM alone\n=\non premise GPU server hour by hour ram need / Available RAM per on premise GPU\nserver instance\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in GB:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] / 270.48\ngigabyte\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1453.846153846154, "y": 1050}, {"color": null, "id": "on premise GPU server hour by hour ram need", "label": "on premise GPU\nserver hour by hour\nram need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour ram need\n=\nHourly Manually defined GPU job average occurrences across usage patterns * RAM\nneeded on server on premise GPU server to process Manually defined GPU job from\ne-footprint hypothesis + no value + Hourly Generative AI model job average\noccurrences across usage patterns * No additional GPU RAM needed because model\nis already loaded in memory from e-footprint hypothesis\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * 50 megabyte +\nno value + 26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01\n17:00:00+00:00 in dimensionless:\nfirst 10 vals [0.05, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.05, 0.05, 0.05],\nlast 10 vals [0.06, 0.03, 0.05, 0.06, 0.07, 0.02, 0.01, 0.01, 0.06, 0.06] * 0\ngigabyte\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in GB:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1462.5, "y": 900}, {"color": null, "id": "Hourly Manually defined GPU job average occurrences across usage patterns", "label": "Hourly Manually\ndefined GPU job\naverage occurrences\nacross usage\npatterns", "shape": "dot", "size": 15, "title": "Hourly Manually defined GPU job average occurrences across usage patterns\n=\nAverage hourly Manually defined GPU job occurrences in usage pattern + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1500.0, "y": 750}, {"color": null, "id": "Average hourly Manually defined GPU job occurrences in usage pattern", "label": "Average hourly\nManually defined GPU\njob occurrences in\nusage pattern", "shape": "dot", "size": 15, "title": "Average hourly Manually defined GPU job occurrences in usage pattern\n=\nHourly Manually defined GPU job occurrences in usage pattern hourly occurrences\naverage Request duration of Manually defined GPU job from e-footprint hypothesis\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8] hourly occurrences average 1 second\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1469.9999999999998, "y": 600}, {"color": null, "id": "Hourly Manually defined GPU job occurrences in usage pattern", "label": "Hourly Manually\ndefined GPU job\noccurrences in usage\npattern", "shape": "dot", "size": 15, "title": "Hourly Manually defined GPU job occurrences in usage pattern\n=\nusage pattern UTC shifted by no value + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8] shifted by no value + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8]", "x": -1443.75, "y": 450}, {"color": null, "id": "usage pattern UTC", "label": "usage pattern UTC", "shape": "dot", "size": 15, "title": "usage pattern UTC\n=\nusage pattern hourly nb of visits from e-footprint hypothesis converted to UTC\nfrom devices country timezone from user data\n=\n26299 values from 2025-01-01 00:00:00 to 2028-01-01 18:00:00 in dimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8] converted to UTC from Europe/Paris\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8]", "x": -1312.5, "y": 300}, {"color": "darkred", "id": "usage pattern hourly nb of visits from e-footprint hypothesis", "label": "usage pattern hourly\nnb of visits from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "usage pattern hourly nb of visits from e-footprint hypothesis = 26299 values\nfrom 2025-01-01 00:00:00 to 2028-01-01 18:00:00 in dimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8]", "x": -1181.25, "y": 150}, {"color": "gold", "id": "devices country timezone from user data", "label": "devices country\ntimezone from user\ndata", "shape": "dot", "size": 15, "title": "devices country timezone from user data = Europe/Paris", "x": -393.75, "y": 150}, {"color": null, "id": "no value", "label": "no value", "shape": "dot", "size": 15, "title": "no value = no value", "x": -787.5, "y": 300}, {"color": "darkred", "id": "Request duration of Manually defined GPU job from e-footprint hypothesis", "label": "Request duration of\nManually defined GPU\njob from e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Request duration of Manually defined GPU job from e-footprint hypothesis = 1\nsecond", "x": -1181.25, "y": 450}, {"color": "darkred", "id": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "label": "RAM needed on server\non premise GPU\nserver to process\nManually defined GPU\njob from e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "RAM needed on server on premise GPU server to process Manually defined GPU job\nfrom e-footprint hypothesis = 50 megabyte", "x": -1350.0, "y": 750}, {"color": null, "id": "Hourly Generative AI model job average occurrences across usage patterns", "label": "Hourly Generative AI\nmodel job average\noccurrences across\nusage patterns", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job average occurrences across usage patterns\n=\nAverage hourly Generative AI model job occurrences in usage pattern + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.05, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.05, 0.05, 0.05],\nlast 10 vals [0.06, 0.03, 0.05, 0.06, 0.07, 0.02, 0.01, 0.01, 0.06, 0.06] + no\nvalue\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.05, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.05, 0.05, 0.05],\nlast 10 vals [0.06, 0.03, 0.05, 0.06, 0.07, 0.02, 0.01, 0.01, 0.06, 0.06]", "x": -1200.0, "y": 750}, {"color": null, "id": "Average hourly Generative AI model job occurrences in usage pattern", "label": "Average hourly\nGenerative AI model\njob occurrences in\nusage pattern", "shape": "dot", "size": 15, "title": "Average hourly Generative AI model job occurrences in usage pattern\n=\nHourly Generative AI model job occurrences in usage pattern hourly occurrences\naverage Generative AI model job request duration\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8] hourly occurrences average 28.15\nsecond\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.05, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.05, 0.05, 0.05],\nlast 10 vals [0.06, 0.03, 0.05, 0.06, 0.07, 0.02, 0.01, 0.01, 0.06, 0.06]", "x": -1259.9999999999998, "y": 600}, {"color": null, "id": "Hourly Generative AI model job occurrences in usage pattern", "label": "Hourly Generative AI\nmodel job\noccurrences in usage\npattern", "shape": "dot", "size": 15, "title": "Hourly Generative AI model job occurrences in usage pattern\n=\nusage pattern UTC shifted by no value + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8] shifted by no value + no value\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [7, 2, 2, 4, 2, 1, 1, 6, 7, 7],\nlast 10 vals [8, 4, 7, 8, 9, 2, 1, 1, 8, 8]", "x": -918.75, "y": 450}, {"color": null, "id": "Generative AI model job request duration", "label": "Generative AI model\njob request duration", "shape": "dot", "size": 15, "title": "Generative AI model job request duration\n=\nGenerative AI model job output token count from e-footprint hypothesis * (GPU\nlatency per active parameter and output token from Ecologits * open-mistral-7b\nfrom mistralai nb of active parameters from Ecologits + Base GPU latency per\noutput_token from Ecologits)\n=\n1000 dimensionless * (0.0 second * 7300000000.0 dimensionless + 0.02 second)\n=\n28.15 second", "x": -656.25, "y": 450}, {"color": "darkred", "id": "Generative AI model job output token count from e-footprint hypothesis", "label": "Generative AI model\njob output token\ncount from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model job output token count from e-footprint hypothesis = 1000\ndimensionless", "x": -262.5, "y": 300}, {"color": "darkred", "id": "GPU latency per active parameter and output token from Ecologits", "label": "GPU latency per\nactive parameter and\noutput token from\nEcologits", "shape": "dot", "size": 15, "title": "GPU latency per active parameter and output token from Ecologits = 0.0 second", "x": 262.5, "y": 300}, {"color": null, "id": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "label": "open-mistral-7b from\nmistralai nb of\nactive parameters\nfrom Ecologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai nb of active parameters from Ecologits\n=\nmistralai from e-footprint hypothesis query EcoLogits data with mistralai model\nused from e-footprint hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": 787.5, "y": 300}, {"color": "darkred", "id": "mistralai from e-footprint hypothesis", "label": "mistralai from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "mistralai from e-footprint hypothesis = mistralai", "x": 393.75, "y": 150}, {"color": "darkred", "id": "mistralai model used from e-footprint hypothesis", "label": "mistralai model used\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "mistralai model used from e-footprint hypothesis = open-mistral-7b", "x": 1181.25, "y": 150}, {"color": "darkred", "id": "Base GPU latency per output_token from Ecologits", "label": "Base GPU latency per\noutput_token from\nEcologits", "shape": "dot", "size": 15, "title": "Base GPU latency per output_token from Ecologits = 0.02 second", "x": 1312.5, "y": 300}, {"color": "darkred", "id": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "label": "No additional GPU\nRAM needed because\nmodel is already\nloaded in memory\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "No additional GPU RAM needed because model is already loaded in memory from\ne-footprint hypothesis = 0 gigabyte", "x": -1050.0, "y": 750}, {"color": null, "id": "Available RAM per on premise GPU server instance", "label": "Available RAM per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available RAM per on premise GPU server instance\n=\non premise GPU server RAM * on premise GPU server utilization rate - Occupied\nRAM per on premise GPU server instance including services\n=\n320.0 gigabyte * 0.9 dimensionless - 17.52 gigabyte\n=\n270.48 gigabyte", "x": -1237.5, "y": 900}, {"color": null, "id": "on premise GPU server RAM", "label": "on premise GPU\nserver RAM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM\n=\non premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n* Nb gpus of on premise GPU server from e-footprint hypothesis\n=\n80.0 gigabyte / gpu * 4 gpu\n=\n320.0 gigabyte", "x": -900.0, "y": 750}, {"color": "darkred", "id": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver RAM per GPU\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM\n= 80.0 gigabyte / gpu", "x": -1050.0, "y": 600}, {"color": "darkred", "id": "Nb gpus of on premise GPU server from e-footprint hypothesis", "label": "Nb gpus of on\npremise GPU server\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Nb gpus of on premise GPU server from e-footprint hypothesis = 4 gpu", "x": -840.0, "y": 600}, {"color": null, "id": "on premise GPU server utilization rate", "label": "on premise GPU\nserver utilization\nrate", "shape": "dot", "size": 15, "title": "on premise GPU server utilization rate = 0.9 dimensionless", "x": -750.0, "y": 750}, {"color": null, "id": "Occupied RAM per on premise GPU server instance including services", "label": "Occupied RAM per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied RAM per on premise GPU server instance including services\n=\nBase RAM consumption of on premise GPU server from e-footprint hypothesis +\nGenerative AI model base RAM consumption\n=\n0 gigabyte + 17.52 gigabyte\n=\n17.52 gigabyte", "x": -600.0, "y": 750}, {"color": "darkred", "id": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "label": "Base RAM consumption\nof on premise GPU\nserver from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Base RAM consumption of on premise GPU server from e-footprint hypothesis = 0\ngigabyte", "x": -629.9999999999999, "y": 600}, {"color": null, "id": "Generative AI model base RAM consumption", "label": "Generative AI model\nbase RAM consumption", "shape": "dot", "size": 15, "title": "Generative AI model base RAM consumption\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai total nb of parameters from Ecologits\n* Generative AI model nb of bits per parameter from e-footprint hypothesis\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16 dimensionless\n=\n17.52 gigabyte", "x": -420.0, "y": 600}, {"color": "darkred", "id": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "label": "Generative AI model\nratio between GPU\nmemory footprint and\nmodel size from\nEcologits", "shape": "dot", "size": 15, "title": "Generative AI model ratio between GPU memory footprint and model size from\nEcologits = 1.2 dimensionless", "x": -393.75, "y": 450}, {"color": null, "id": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "label": "open-mistral-7b from\nmistralai total nb\nof parameters from\nEcologits", "shape": "dot", "size": 15, "title": "open-mistral-7b from mistralai total nb of parameters from Ecologits\n=\nmistralai from e-footprint hypothesis query EcoLogits data with mistralai model\nused from e-footprint hypothesis\n=\nmistralai query EcoLogits data with open-mistral-7b\n=\n7300000000.0 dimensionless", "x": -131.25, "y": 450}, {"color": "darkred", "id": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "label": "Generative AI model\nnb of bits per\nparameter from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Generative AI model nb of bits per parameter from e-footprint hypothesis = 16\ndimensionless", "x": 131.25, "y": 450}, {"color": null, "id": "Raw nb of on premise GPU server instances based on CPU alone", "label": "Raw nb of on premise\nGPU server instances\nbased on CPU alone", "shape": "dot", "size": 15, "title": "Raw nb of on premise GPU server instances based on CPU alone\n=\non premise GPU server hour by hour compute need / Available CPU per on premise\nGPU server instance\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in gpu:\nfirst 10 vals [0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01],\nlast 10 vals [0.02, 0.01, 0.01, 0.02, 0.02, 0.0, 0.0, 0.0, 0.02, 0.02] / 3.6 gpu\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]", "x": -1211.5384615384614, "y": 1050}, {"color": null, "id": "on premise GPU server hour by hour compute need", "label": "on premise GPU\nserver hour by hour\ncompute need", "shape": "dot", "size": 15, "title": "on premise GPU server hour by hour compute need\n=\nHourly Manually defined GPU job average occurrences across usage patterns * gpus\nneeded on server on premise GPU server to process Manually defined GPU job from\ne-footprint hypothesis + no value + Hourly Generative AI model job average\noccurrences across usage patterns * Generative AI model job nb of required GPUs\nduring inference\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in\ndimensionless:\nfirst 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\nlast 10 vals [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * 1 gpu + no\nvalue + 26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00\nin dimensionless:\nfirst 10 vals [0.05, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.05, 0.05, 0.05],\nlast 10 vals [0.06, 0.03, 0.05, 0.06, 0.07, 0.02, 0.01, 0.01, 0.06, 0.06] * 0.22\ngpu\n=\n26296 values from 2024-12-31 23:00:00+00:00 to 2028-01-01 17:00:00+00:00 in gpu:\nfirst 10 vals [0.01, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.01, 0.01, 0.01],\nlast 10 vals [0.02, 0.01, 0.01, 0.02, 0.02, 0.0, 0.0, 0.0, 0.02, 0.02]", "x": -1012.5, "y": 900}, {"color": "darkred", "id": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "label": "gpus needed on\nserver on premise\nGPU server to\nprocess Manually\ndefined GPU job from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "gpus needed on server on premise GPU server to process Manually defined GPU job\nfrom e-footprint hypothesis = 1 gpu", "x": -450.0, "y": 750}, {"color": null, "id": "Generative AI model job nb of required GPUs during inference", "label": "Generative AI model\njob nb of required\nGPUs during\ninference", "shape": "dot", "size": 15, "title": "Generative AI model job nb of required GPUs during inference\n=\nGenerative AI model ratio between GPU memory footprint and model size from\nEcologits * open-mistral-7b from mistralai nb of active parameters from\nEcologits * Generative AI model nb of bits per parameter from e-footprint\nhypothesis / on premise GPU server RAM per GPU from Estimating the Carbon\nFootprint of BLOOM\n=\n1.2 dimensionless * 7300000000.0 dimensionless * 16 dimensionless / 80.0\ngigabyte / gpu\n=\n0.22 gpu", "x": -300.0, "y": 750}, {"color": null, "id": "Available CPU per on premise GPU server instance", "label": "Available CPU per on\npremise GPU server\ninstance", "shape": "dot", "size": 15, "title": "Available CPU per on premise GPU server instance\n=\nNb gpus of on premise GPU server from e-footprint hypothesis * on premise GPU\nserver utilization rate - Occupied CPU per on premise GPU server instance\nincluding services\n=\n4 gpu * 0.9 dimensionless - 0 gpu\n=\n3.6 gpu", "x": -787.5, "y": 900}, {"color": null, "id": "Occupied CPU per on premise GPU server instance including services", "label": "Occupied CPU per on\npremise GPU server\ninstance including\nservices", "shape": "dot", "size": 15, "title": "Occupied CPU per on premise GPU server instance including services\n=\nBase gpu consumption of on premise GPU server from e-footprint hypothesis + no\nvalue\n=\n0 gpu + no value\n=\n0 gpu", "x": 0.0, "y": 750}, {"color": "darkred", "id": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "label": "Base gpu consumption\nof on premise GPU\nserver from\ne-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Base gpu consumption of on premise GPU server from e-footprint hypothesis = 0\ngpu", "x": 420.0, "y": 600}, {"color": null, "id": "User defined number of on premise GPU server instances", "label": "User defined number\nof on premise GPU\nserver instances", "shape": "dot", "size": 15, "title": "User defined number of on premise GPU server instances = no value", "x": -787.5, "y": 1200}, {"color": "darkred", "id": "Server type of on premise GPU server from e-footprint hypothesis", "label": "Server type of on\npremise GPU server\nfrom e-footprint\nhypothesis", "shape": "dot", "size": 15, "title": "Server type of on premise GPU server from e-footprint hypothesis = on-premise", "x": -900.0, "y": 1350}, {"color": null, "id": "on premise GPU server idle power", "label": "on premise GPU\nserver idle power", "shape": "dot", "size": 15, "title": "on premise GPU server idle power\n=\non premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM * Nb gpus of on premise GPU server from e-footprint hypothesis\n=\n50.0 watt / gpu * 4 gpu\n=\n200.0 watt", "x": -1260.0, "y": 1500}, {"color": "darkred", "id": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU idle\npower from\nEstimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of\nBLOOM = 50.0 watt / gpu", "x": -450.0, "y": 1350}, {"color": null, "id": "PUE of on premise GPU server", "label": "PUE of on premise\nGPU server", "shape": "dot", "size": 15, "title": "PUE of on premise GPU server = 1.2 dimensionless", "x": -630.0, "y": 1500}, {"color": null, "id": "one hour", "label": "one hour", "shape": "dot", "size": 15, "title": "one hour = 1 hour", "x": 0.0, "y": 1500}, {"color": null, "id": "on premise GPU server power", "label": "on premise GPU\nserver power", "shape": "dot", "size": 15, "title": "on premise GPU server power\n=\non premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM *\nNb gpus of on premise GPU server from e-footprint hypothesis\n=\n400.0 watt / gpu * 4 gpu\n=\n1600.0 watt", "x": 1260.0, "y": 1500}, {"color": "darkred", "id": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "label": "on premise GPU\nserver GPU power\nfrom Estimating the\nCarbon Footprint of\nBLOOM", "shape": "dot", "size": 15, "title": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM =\n400.0 watt / gpu", "x": 900.0, "y": 1350}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Hourly number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on RAM alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "no value", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on CPU alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "no value", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "no value"}, {"arrows": "to", "from": "User defined number of on premise GPU server instances", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "Server type of on premise GPU server from e-footprint hypothesis", "to": "Hourly number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server idle power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "PUE of on premise GPU server", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "one hour", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Hourly raw number of on premise GPU server instances", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on RAM alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour ram need", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "RAM needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "no value", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "No additional GPU RAM needed because model is already loaded in memory from e-footprint hypothesis", "to": "on premise GPU server hour by hour ram need"}, {"arrows": "to", "from": "Available RAM per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on RAM alone"}, {"arrows": "to", "from": "on premise GPU server RAM", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server RAM"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied RAM per on premise GPU server instance including services", "to": "Available RAM per on premise GPU server instance"}, {"arrows": "to", "from": "Base RAM consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model base RAM consumption", "to": "Occupied RAM per on premise GPU server instance including services"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "open-mistral-7b from mistralai total nb of parameters from Ecologits", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai total nb of parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model base RAM consumption"}, {"arrows": "to", "from": "Raw nb of on premise GPU server instances based on CPU alone", "to": "Hourly raw number of on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server hour by hour compute need", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Hourly Manually defined GPU job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Manually defined GPU job occurrences in usage pattern", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Manually defined GPU job occurrences in usage pattern", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "Request duration of Manually defined GPU job from e-footprint hypothesis", "to": "Average hourly Manually defined GPU job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Manually defined GPU job average occurrences across usage patterns"}, {"arrows": "to", "from": "gpus needed on server on premise GPU server to process Manually defined GPU job from e-footprint hypothesis", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "no value", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Hourly Generative AI model job average occurrences across usage patterns", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Average hourly Generative AI model job occurrences in usage pattern", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Hourly Generative AI model job occurrences in usage pattern", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern UTC", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "usage pattern hourly nb of visits from e-footprint hypothesis", "to": "usage pattern UTC"}, {"arrows": "to", "from": "devices country timezone from user data", "to": "usage pattern UTC"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job request duration", "to": "Average hourly Generative AI model job occurrences in usage pattern"}, {"arrows": "to", "from": "Generative AI model job output token count from e-footprint hypothesis", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "GPU latency per active parameter and output token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Base GPU latency per output_token from Ecologits", "to": "Generative AI model job request duration"}, {"arrows": "to", "from": "no value", "to": "Hourly Generative AI model job average occurrences across usage patterns"}, {"arrows": "to", "from": "Generative AI model job nb of required GPUs during inference", "to": "on premise GPU server hour by hour compute need"}, {"arrows": "to", "from": "Generative AI model ratio between GPU memory footprint and model size from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "open-mistral-7b from mistralai nb of active parameters from Ecologits", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "mistralai from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "mistralai model used from e-footprint hypothesis", "to": "open-mistral-7b from mistralai nb of active parameters from Ecologits"}, {"arrows": "to", "from": "Generative AI model nb of bits per parameter from e-footprint hypothesis", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "on premise GPU server RAM per GPU from Estimating the Carbon Footprint of BLOOM", "to": "Generative AI model job nb of required GPUs during inference"}, {"arrows": "to", "from": "Available CPU per on premise GPU server instance", "to": "Raw nb of on premise GPU server instances based on CPU alone"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "on premise GPU server utilization rate", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Occupied CPU per on premise GPU server instance including services", "to": "Available CPU per on premise GPU server instance"}, {"arrows": "to", "from": "Base gpu consumption of on premise GPU server from e-footprint hypothesis", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "Occupied CPU per on premise GPU server instance including services"}, {"arrows": "to", "from": "no value", "to": "no value"}, {"arrows": "to", "from": "on premise GPU server power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server power"}, {"arrows": "to", "from": "on premise GPU server idle power", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "on premise GPU server GPU idle power from Estimating the Carbon Footprint of BLOOM", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "Nb gpus of on premise GPU server from e-footprint hypothesis", "to": "on premise GPU server idle power"}, {"arrows": "to", "from": "PUE of on premise GPU server", "to": "Hourly energy consumed by on premise GPU server instances"}, {"arrows": "to", "from": "one hour", "to": "Hourly energy consumed by on premise GPU server instances"}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": false,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>